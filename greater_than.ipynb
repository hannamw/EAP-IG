{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the Greater-Than Circuit Using EAP(-IG)\n",
    "\n",
    "First, we import various packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import PreTrainedTokenizer\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "from eap.graph import Graph\n",
    "from eap.evaluate import evaluate_graph, evaluate_baseline\n",
    "from eap.attribute import attribute "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Metrics\n",
    "\n",
    "This package expects data to come from a dataloader. Each item consists of clean and corrupted paired inputs (strings), as well as a label (encoded as a token id). For convenience, we've included a dataset in that form as a CSV (more to come with the full code of the paper).\n",
    "\n",
    "A metric takes in the model's (possibly corrupted) logits, clean logits, input lengths, and labels. It computes a metric value for each batch item; this can either be used as is, or turned into a loss (lower is better), or meaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_EAP(xs):\n",
    "    clean, corrupted, labels = zip(*xs)\n",
    "    clean = list(clean)\n",
    "    corrupted = list(corrupted)\n",
    "    return clean, corrupted, labels\n",
    "\n",
    "class EAPDataset(Dataset):\n",
    "    def __init__(self, filepath):\n",
    "        self.df = pd.read_csv(filepath)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1)\n",
    "\n",
    "    def head(self, n: int):\n",
    "        self.df = self.df.head(n)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        return row['clean'], row['corrupted'], row['label']\n",
    "    \n",
    "    def to_dataloader(self, batch_size: int):\n",
    "        return DataLoader(self, batch_size=batch_size, collate_fn=collate_EAP)\n",
    "    \n",
    "def get_logit_positions(logits: torch.Tensor, input_length: torch.Tensor):\n",
    "    batch_size = logits.size(0)\n",
    "    idx = torch.arange(batch_size, device=logits.device)\n",
    "\n",
    "    logits = logits[idx, input_length - 1]\n",
    "    return logits\n",
    "\n",
    "def get_prob_diff(tokenizer: PreTrainedTokenizer):\n",
    "    year_indices = torch.tensor([tokenizer(f'{year:02d}').input_ids[0] for year in range(100)])\n",
    "\n",
    "    def prob_diff(logits: torch.Tensor, clean_logits: torch.Tensor, input_length: torch.Tensor, labels: torch.Tensor, mean=True, loss=False):\n",
    "        logits = get_logit_positions(logits, input_length)\n",
    "        probs = torch.softmax(logits, dim=-1)[:, year_indices]\n",
    "\n",
    "        results = []\n",
    "        for prob, year in zip(probs, labels):\n",
    "            results.append(prob[year + 1 :].sum() - prob[: year + 1].sum())\n",
    "    \n",
    "        results = torch.stack(results)\n",
    "        if loss:\n",
    "            results = -results\n",
    "        if mean: \n",
    "            results = results.mean()\n",
    "        return results\n",
    "    return prob_diff\n",
    "\n",
    "def kl_div(logits: torch.Tensor, clean_logits: torch.Tensor, input_length: torch.Tensor, labels: torch.Tensor, mean=True, loss=True):\n",
    "    logits = get_logit_positions(logits, input_length)\n",
    "    clean_logits = get_logit_positions(clean_logits, input_length)\n",
    "\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    clean_probs = torch.softmax(clean_logits, dim=-1)\n",
    "\n",
    "    results = kl_div(probs.log(), clean_probs.log(), log_target=True, reduction='none').mean(-1)\n",
    "    return results.mean() if mean else results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing EAP-IG\n",
    "\n",
    "First, we load the model, data, and metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_name = 'gpt2-small'\n",
    "model = HookedTransformer.from_pretrained(model_name, device='cuda')\n",
    "model.cfg.use_split_qkv_input = True\n",
    "model.cfg.use_attn_result = True\n",
    "model.cfg.use_hook_mlp_in = True\n",
    "\n",
    "ds = EAPDataset('greater_than_data.csv')\n",
    "dataloader = ds.to_dataloader(120)\n",
    "prob_diff = get_prob_diff(model.tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we perform EAP! We instantiate an unscored graph from the model, and use the attribute method to score it. This requires a model, graph, dataloader, and loss. We set `method='EAP-IG'`, and set the number of iterations via `ig_steps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]/home/Ubuntu/miniconda3/envs/eap-ig-test/lib/python3.10/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "100%|██████████| 9/9 [00:10<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a graph with a model\n",
    "g = Graph.from_model(model)\n",
    "\n",
    "# Attribute using the model, graph, clean / corrupted data and labels, as well as a metric\n",
    "attribute(model, g, dataloader, partial(prob_diff, loss=True, mean=True), method='EAP-IG', ig_steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply greedy search to the scored graph to find a circuit! We prune dead nodes, and export the circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.apply_topn(200, absolute=True)\n",
    "g.prune_dead_nodes()\n",
    "g.to_json('graph.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then convert our circuit into a visualization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gz = g.to_graphviz()\n",
    "gz.draw(f'graph.png', prog='dot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then evaluate our model's metric score as opposed to a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:01<00:00,  8.76it/s]\n",
      "100%|██████████| 9/9 [00:01<00:00,  8.76it/s]\n",
      "100%|██████████| 9/9 [00:01<00:00,  5.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original performance was 0.8143223524093628; the circuit's performance is 0.742073655128479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "baseline = evaluate_baseline(model, dataloader, partial(prob_diff, loss=False, mean=False)).mean().item()\n",
    "results = evaluate_graph(model, g, dataloader, partial(prob_diff, loss=False, mean=False)).mean().item()\n",
    "print(f\"Original performance was {baseline}; the circuit's performance is {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare that to a circuit found with vanilla EAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.65it/s]\n",
      "100%|██████████| 9/9 [00:01<00:00,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original performance was 0.8143223524093628; the circuit's performance is 0.6777775883674622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a graph with a model\n",
    "g_eap = Graph.from_model(model)\n",
    "\n",
    "# Attribute using the model, graph, clean / corrupted data and labels, as well as a metric\n",
    "attribute(model, g_eap, dataloader, partial(prob_diff, loss=True, mean=True), method='EAP')\n",
    "\n",
    "g_eap.apply_greedy(200)\n",
    "g_eap.prune_dead_nodes()\n",
    "\n",
    "results_eap = evaluate_graph(model, g_eap, dataloader, partial(prob_diff, loss=False, mean=False)).mean().item()\n",
    "print(f\"Original performance was {baseline}; the circuit's performance is {results_eap}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also test other EAP-IG variants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/84 [00:00<?, ?it/s]/home/Ubuntu/miniconda3/envs/eap-ig-test/lib/python3.10/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "100%|██████████| 84/84 [00:14<00:00,  5.78it/s]\n",
      "100%|██████████| 32491/32491 [00:00<00:00, 769062.64it/s]\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original performance was 0.8143222332000732; the circuit's performance is 0.6988648772239685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a graph with a model\n",
    "g_cc = Graph.from_model(model)\n",
    "\n",
    "# Attribute using the model, graph, clean / corrupted data and labels, as well as a metric\n",
    "attribute(model, g_cc, dataloader, partial(prob_diff, loss=True, mean=True), method='clean-corrupted')\n",
    "\n",
    "g_cc.apply_greedy(200)\n",
    "g_cc.prune_dead_nodes()\n",
    "\n",
    "results_cc = evaluate_graph(model, g_cc, dataloader, partial(prob_diff, loss=False, mean=False)).mean().item()\n",
    "print(f\"Original performance was {baseline}; the circuit's performance is {results_cc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/84 [00:00<?, ?it/s]/home/Ubuntu/miniconda3/envs/eap-ig-test/lib/python3.10/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "100%|██████████| 84/84 [00:33<00:00,  2.49it/s]\n",
      "100%|██████████| 32491/32491 [00:00<00:00, 634147.97it/s]\n",
      "100%|██████████| 84/84 [00:04<00:00, 16.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original performance was 0.8143222332000732; the circuit's performance is 0.18004724383354187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a graph with a model\n",
    "g_eap_p_act = Graph.from_model(model)\n",
    "\n",
    "# Attribute using the model, graph, clean / corrupted data and labels, as well as a metric\n",
    "attribute(model, g_eap_p_act, dataloader, partial(prob_diff, loss=True, mean=True), method='EAP-IG-partial-activations')\n",
    "\n",
    "g_eap_p_act.apply_greedy(200)\n",
    "g_eap_p_act.prune_dead_nodes()\n",
    "\n",
    "results_eap_p_act = evaluate_graph(model, g_eap_p_act, dataloader, partial(prob_diff, loss=False, mean=False)).mean().item()\n",
    "print(f\"Original performance was {baseline}; the circuit's performance is {results_eap_p_act}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/84 [00:00<?, ?it/s]/home/Ubuntu/miniconda3/envs/eap-ig-test/lib/python3.10/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "100%|██████████| 84/84 [20:18<00:00, 14.50s/it]\n",
      "100%|██████████| 32491/32491 [00:00<00:00, 760046.69it/s]\n",
      "100%|██████████| 84/84 [00:04<00:00, 17.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original performance was 0.8143222332000732; the circuit's performance is 0.6452323198318481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a graph with a model\n",
    "g_eap_activations = Graph.from_model(model)\n",
    "\n",
    "# Attribute using the model, graph, clean / corrupted data and labels, as well as a metric\n",
    "attribute(model, g_eap_activations, dataloader, partial(prob_diff, loss=True, mean=True), method='EAP-IG-activations')\n",
    "\n",
    "g_eap_activations.apply_greedy(200)\n",
    "g_eap_activations.prune_dead_nodes()\n",
    "\n",
    "results_eap_activations = evaluate_graph(model, g_eap_activations, dataloader, partial(prob_diff, loss=False, mean=False)).mean().item()\n",
    "print(f\"Original performance was {baseline}; the circuit's performance is {results_eap_activations}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agreement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
